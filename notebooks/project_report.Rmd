---
title: "Finnish Water Level"
output: pdf_document
include: pdf_document
---
\newpage

\section * {Appendix}

## Pre-processing script (written in Python)

```
import os.path

import pandas as pd


def process_data(data_path, mean_sea_level_path, output_file_path, yearly_means_output_path):
    print(f"Pre-processing file: {data_path}")

    data = pd.read_csv(data_path)
    mean_sea_level_data = pd.read_csv(mean_sea_level_path)

    # if file exist then don't create a new one and read from it only
    if os.path.isfile(output_file_path):
        n_data = pd.read_csv(output_file_path)
        print(f"Read normalized data from {output_file_path}")
    else:
        n_data = data.copy()

        for i, row in n_data.iterrows():
            # FIXME: Small hack : to show the progress bar
            if i % 1000 == 0:
                print('\r ready: %.3f%%' % (i / len(n_data) * 100), end=" ")

            # main normalizing step, N2000
            # water_level - mean sea level data for that particular year
            n_data.iat[i, 5] = row['Water level (mm)'] - 
                               mean_sea_level_data.loc[mean_sea_level_data['Year'] ==
                               row['Year']].values[0][1]

        n_data = n_data.rename(columns={'Water level (mm)': 'water_level'})

        n_data.to_csv(output_file_path)

        print(f"\nWrote output to {output_file_path}")

    # finding the mean value of each year
    yearly_means = n_data["water_level"].groupby(n_data["Year"]).mean().dropna()

    with open(yearly_means_output_path, "w") as f:
        f.writelines(pd.Series.to_csv(yearly_means))

    return yearly_means


if __name__ == '__main__':
    data_path = r"../data/Kemi/Kemi.csv"
    mean_sea_level_path = r"../data/Kemi/Kemi_mw_n2000.csv"
    normalized_output_path = r"../data/Kemi/Kemi_normalized.csv"
    yearly_means_output_path = r"../data/Kemi/Kemi_yearly_means.csv"

    yearly_mean = process_data(data_path, 
                              mean_sea_level_path, 
                              normalized_output_path, 
                              yearly_means_output_path)

    print(yearly_mean)

```

## R Code Below

```{r message=FALSE, warning=FALSE, results='hide'}
library(rstan)
library(ggplot2)
library(dplyr)
library(bayesplot)
library(loo)
library(matrixStats)
```

## Setting all the paths

```{r warning=FALSE, results='hide', message=FALSE}
main_data_dir = "../data"
separate_stan_model = "../model/separate.stan"
hierarchical_stan_model = "../model/hierarchical.stan"
```

## Loading data once it has been pre\_processed

```{r warning=FALSE, results='hide', message=FALSE}
combineListsAsOne <-function(list1, list2){
  n <- c()
  for(x in list1){
    n<-c(n, x)
  }
  for(y in list2){
    n<-c(n, y)
  }
  return(n)
}
number_of_cities = 0
x = list()
y = list()
ii = list()
city_names = list()
for (data_file in list.files(main_data_dir, 
                             pattern="*_yearly_means.csv", 
                             full.names=TRUE, 
                             recursive = TRUE)){
  
  preprocessed_data = read.csv(data_file, header=FALSE)
  
  c_name = strsplit(strsplit(data_file, "/")[[1]][4],"_")[[1]][1]
  city_names[number_of_cities + 1] = c_name
    
  x = combineListsAsOne(x, preprocessed_data[,1])
  y = combineListsAsOne(y, preprocessed_data[,2])
  number_of_cities = number_of_cities + 1
  
  for (j in seq(preprocessed_data[,1])){
    ii = combineListsAsOne(ii, number_of_cities)
  }
}
```

## Plotting the data

```{r warning=FALSE, results='hide', message=FALSE}
data_plot = list(
    'N'= 49,
    'x'= x[1:49],
    'y'= y[1:49],
    'k' = number_of_cities
)
for (i in seq(1:number_of_cities)){
  x_temp = x
  y_temp = y
  
  a = ((i-1)*49) +1
  b = i*49
}
```

## Preparing data for Stan model

```{r warning=FALSE, results='hide', message=FALSE}
data = list(
    'N'= length(x),
    'x'= x,
    'y'= y,
    'k' = number_of_cities,
    'i' = ii
)
```

## Separate Stan Model

```{r warning=FALSE, results='hide', message=FALSE}
fit_separate = stan(separate_stan_model, data=data)
fit_separate_data = extract(fit_separate, permuted=T)
```

## Plots for all cities with slope line and quantiles after Stan fit for Separate Model

```{r warning=FALSE, results='hide', message=FALSE}
mu = matrix(data = NA, nrow = 49, ncol = 4000)

for(i in seq(1:number_of_cities)){
  
  a = ((i-1)*49) +1
  b = i*49
  
  x_temp = x[a:b]
  y_temp = y[a:b]
  
  for(t in seq(from = 1970, to =2019, by =1)){
    mu[t-1970,] =  fit_separate_data$alpha[,i] + ((t-1970) * 
                                                    
                                                    fit_separate_data$beta[,i])
  }

  mu_quan = colQuantiles(t(mu), probs = c(0.05, 0.5, 0.95))
  
  df_post = data.frame(quan_05=mu_quan[,1],
                       
                       quan_50=mu_quan[,2],
                       
                       quan_95=mu_quan[,3],
                       
                       x=x_temp)
}
```

## LOO Fitting for Separate Model

```{r warning=FALSE, results='hide', message=FALSE}
loo_separate_data = loo(fit_separate)
```

## Hierarchical Stan Model

```{r warning=FALSE, results='hide', message=FALSE}
fit_hierarchical = stan(hierarchical_stan_model, data=data)
fit_hierarchical_data = extract(fit_hierarchical, permuted=T)
```

## Plots for all cities with slope line and quantiles after Stan fit for Hierarchical Model

```{r warning=FALSE, results='hide', message=FALSE}
mu = matrix(data = NA, nrow = 49, ncol = 4000)

for(i in seq(1:number_of_cities)){
  
  a = ((i-1)*49) +1
  b = i*49
  
  x_temp = x[a:b]
  y_temp = y[a:b]
  
  for(t in seq(from = 1970, to =2019, by =1)){
    mu[t-1970,] =  fit_hierarchical_data$alpha[,i] + ((t-1970) * fit_hierarchical_data$beta[,i])
  }

  mu_quan = colQuantiles(t(mu), probs = c(0.05, 0.5, 0.95))
  
  df_post = data.frame(quan_05=mu_quan[,1], quan_50=mu_quan[,2], quan_95=mu_quan[,3], x=x_temp)
}
```

## Loo
```{r warning=FALSE, results='hide', message=FALSE}
loo_hierarchical_data = loo(fit_hierarchical)
```

## Posterior Predictive Checking for Separate Model

```{r warning=FALSE, results='hide', message=FALSE}
y_rep <- as.matrix(fit_separate, pars = "y_rep")
  
for (i in seq(1:number_of_cities)){
  x_temp = x
  y_temp = y
  
  a = ((i-1)*49) +1
  b = i*49
}
```

## Posterior Predictive Checking for Hierarchical Model

```{r warning=FALSE, results='hide', message=FALSE}
y_rep <- as.matrix(fit_hierarchical, pars = "y_rep")
  
for (i in seq(1:number_of_cities)){
  x_temp = x
  y_temp = y
  
  a = ((i-1)*49) +1
  b = i*49
}
```


### Model Checking -- Not sure if this is the correct way or ELPD one.

```{r warning=FALSE, results='hide', message=FALSE}
y_rep_draws_hierarchical <- as.matrix(fit_hierarchical, pars = "alpha")
y_rep_draws_separate <- as.matrix(fit_separate, pars = "alpha")
y_reps <- cbind(separate = y_rep_draws_separate[, 1],
                 hierarchical = y_rep_draws_hierarchical[, 1])
mcmc_areas(y_reps, prob = 0.95)
```

```{r warning=FALSE, results='hide', message=FALSE}
compare(loo_hierarchical_data, loo_separate_data)
```




\newpage
\section{Introduction}
Today, one of the biggest threat to the society is global warming. Sea level rise is the immediate result of global warming as it melts the ice sheets and  expands the water volume. Cities like Venice, New Orleans and Osaka are expected to experience major problems with the rise in sea level in near future. 

Venice is already facing major problems. 

\begin{figure}[h!]
\centerline{\includegraphics[width=200pt]{../img/venice.png}}
  \caption{Venice Floods}
  \label{Figure 1}
\end{figure}


Goal of the project is to examine how much the sea level rise affects the coastal cities in Finland. For purpose of the study, four cities from coastal Finland are selected and the data from January, 1970 to November, 2019 is used. Linear regression with Gaussian noise,  $Error \sim N(0, \sigma^2)$, is fitted on the data to check the threat on Finnish cities.

The further sections discuss about the data and its pre-processing followed by a discussion on priors, Stan models and the results obtained. We conclude the report with our findings and possible future work.

\section{Data}

To find out about the sea level rise in Finland we need hostorical data about the same. In Finland, the \href{https://ilmatieteenlaitos.fi/havaintojen-lataus#!/}{Finnish Meteorologigal Institute} provides open data about the weather and it was an easy portal to download the data needed for the study. The data from January, 1970 to November, 2019 was taken for the study. The cities are selected in a way that they are evenly distributed around the coast line so that the study covers the overall situation in Finland. The selected cities are Kemi, Turku, Helsinki and Oulu. 

The data is useful for weather analysis and is available openly so the results can be made public as the data is public. 

\begin{figure}
\centerline{\includegraphics[width=150pt]{../img/map.png}}
  \caption{Cities selected for the study}
  \label{Figure 2}
\end{figure}

\newpage

\subsection{Format of Data}
Data by Finnish Meteorologigal Institute is collected every hour for every day throughout the year, i.e. 24 * 30 * 365 readings are available for each city per year.

\begin{figure}[h]
\centerline{\includegraphics[width=300pt]{../img/data_example.png}}
  \caption{Raw Data}
  \label{Figure 3}
\end{figure}

\subsection{Data pre-processing}

Once the data is downloaded it is essential to know the zero of the scale of measurement i.e. the height system in which the values are given. In Finland there are a number of systems available from which we used N2000.

Thus all the data was normalized using N2000 data which was available on the Finnish Meteorologigal Institute  website,  \href{https://en.ilmatieteenlaitos.fi/theoretical-mean-sea-level}{N2000 data}

Also, the data was too accurate so the mean of sea level for each year was calculated.

The whole pre-processing script can be found in the Appendix section.

\subsection{Processed Data}

After pre-processing the data consists of one mean sea-level value per year. The example can be found in Figure 3. Visual representation of the data is plotted below for all 4 cities.

\begin{figure}[h]
\centerline{\includegraphics[width=300pt]{../img/processed_data.png}}
  \caption{Processed Data}
  \label{Figure 3}
\end{figure}

```{r echo=FALSE}
for (i in seq(1:number_of_cities)){
  x_temp = x
  y_temp = y
  
  a = ((i-1)*49) +1
  b = i*49
  
  print(ggplot() +
    geom_point(aes(x_temp[a:b], y_temp[a:b]), data = data.frame(data_plot), size = 1) +
    labs(y = 'Water level', x= "Year", title = city_names[i]) +
    guides(linetype = F))
}
```


\section{Project Workflow}
In this section a general overview of what is done in the project is provided and it follows the Bayesian Work-flow. The steps are as shown below
\begin{itemize}
    \item Define the Bayesian Models
    \item Perform sensitivity analysis of priors and choose appropriate prior(s)
    \item Define the models and fit the data
    \item Check Convergence Statistics
    \item Perform Posterior Predictive Analysis
    \item Visualize model through plots
    \item Perform PSIS LOO cross validation
    \item Perform Model evaluation
\end{itemize}

\subsection{Models}
A \textbf{gaussian linear model} to the time series data from 1970 to 2019 is fitted. Setting year 1970 as origin which makes the slope parameter relative to year 1970 and the intercept modelled for year 1970. Since measurements were collected from 4 different geographical locations it was decided to try a \textbf{hierarchial model} in addition to a \textbf{separate model}.


The Guassian Linear Model is defined as : 
$y = \beta x + \alpha$
where $\beta$ is the slope, $\alpha$ is a constant and $x$ is the year number. 


Different priors for both the model were tested which is described in the next section. Based on the results of these, suitable weakly informative priors were chosen for the models.

\subsection{Sensitivity Analysis of Priors}
\subsubsection{Informative Priors}
According to \href{https://sealevel.nasa.gov/understanding-sea-level/key-indicators/global-mean-sea-level/}{NASA}, the global mean sea level rise is \textbf{3.3} which can be used as a prior. However, it is a very informative prior so weakly informative priors were defined to fit the models. 
\subsubsection{Weakly Informative Priors}
The weakly informative priors tested are:
\begin{enumerate}
    \item $N(3.3,100)$
    \item $N(0,100)$
    \item $N(0,10)$
\end{enumerate}
However it was decided to use $N(0,10)$ as the prior for both the models.

\section{Model Fitting}
\subsection{Separate Model}
A weakly informative prior which is $N \sim (0,10)$ is being used, so 0 is the prior mean for the slope and 10 is the standard deviation. No prior is given to alpha because the slope should approximate the data as well as possible. 


\begin{figure}
\centerline{\includegraphics[width=300pt]{../img/separatemodel.jpg}}
  \caption{Separte Model}
  \label{Figure 4}
\end{figure}

\subsubsection{Priors}
$p(\beta_g) \sim N(0,10)$
\subsubsection{Likelihood}
$p(y|\theta) \sim N(\alpha_g + \beta_g (\textbf{x}-1970) | \sigma_g)$


\subsection{Hierarchical Model}
Having grouped data from 4 different locations it makes sense to try a hierarchial model. A prior distribution was used, based on the same global data as the seperate model $N \sim (0,10)$. $\beta_0$ is used as a common mean for modelling the slopes for each location. Then another hyperprior $p(\sigma_0) \sim N(0,10)$ is used as a common variance for the slopes.


\begin{figure}
\centerline{\includegraphics[width=300pt]{../img/hierarchicalmodel.jpg}}
  \caption{Separte Model}
  \label{Figure 4}
\end{figure}

\subsubsection{Hyperpriors}
$p(\beta_0) \sim N(0|10)$
\\
$p(\sigma_0) \sim N(0 | 10)$

\subsubsection{Priors}
$p(\beta_g) \sim N(\beta_0 | \sigma_0)$

\subsubsection{Likelihood}
$p(y|\theta) \sim N(\alpha_g + \beta_g (\textbf{x}-1970) | \sigma)$



\subsection{Convergence Statistics in Stan}
\begin{table}[h]
    \centering
    \begin{tabular}{||c|c|c|c|c|c|c|c||}
           \hline
            Variable & mean & se\_mean & sd &2.50\% & 97.50\% & n\_eff & Rhat \\
            \hline
             &  &  & & & & &

    \end{tabular}
    \caption{Sample table layout for convergence statistics}
    \label{tab:my_label}
\end{table}

For each parameter (variable) of the model, the format as shown in Table 1 shows:
\begin{itemize}
    \item The mean column shows the mean for each variable of all the posterior draws of that variable generated in Markov Chain Monte Carlo (MCMC) simulation.
    \item The se$\_$mean column shows standard errors (se) for the posterior means. Standard error of the mean (SEM) depends both on the standard deviation (SD) and the sample size (n): SE = SD/$\sqrt{n}$.
    \item The sd column tells the standard deviation (SD). SD is a measure of variability or dispersion.
    \item The 2.5 $\%$ column reports the value corresponding the lower bound of 95$\%$  confidence interval.
    \item The 97.5 $\%$  column reports the value corresponding the upper bound of 95$\%$  confidence interval.
    \item The n$\_$eff  column tells the effective number of simulation draws, called the effective sample size, n$\_$eff
    \item The Rhat column reports us the values of Rˆ that is our convergence statistic, Rˆ. Here we will consider the threshold of 1.01 for the condition of Rˆ being ’near’ 1 . If Rˆ $>$ 1.01, we consider that the chains have not converged, while if Rˆ $<$ 1.01 we consider that the chains have probably converged and estimates are reliable.
\end{itemize}

The Results section will show the detail of the convergence statistics for both Separate and Hierarchical model. 

\subsection{Leave One out cross validation}
Leave-one-out cross-validation (LOO-CV or LOO) is a method to evaluate the predictive performance of
fitted models for a data set. LOO-CV is an approach for estimating pointwise out-of-sample prediction
accuracy from the fitted models using the log-likelihood assessed at the posterior simulations of the parameter
values.

Here we use Pareto smoothed importance sampling (PSIS) LOO (PSIS-LOO) method for computing ap-
proximate LOO-CV given the posterior draws of the parameters. PSIS is a new approach that makes it
possible to compute LOO using importance weights that would otherwise be unstable. PSIS fits a Pareto
distribution to the upper tail of the distribution of the importance weights, and in this way provides relatively
accurate and reliable estimate.

PSIS-LOO estimate is the sum of the LOO log predictive densities. The reliability of the PSIS-LOO estimates
are assessed for a fitted model based on the k-values that are the estimated Pareto tail indices. The PSIS-LOO
estimate can be considered reliable if all k-values are k $<$ 0.5. Otherwise, we have a concern that the
PSIS-LOO estimate may be biased i.e. it is possible that the estimate is too optimistic, overestimating the
predictive accuracy of the fitted model.

When we are comparing different models which have the same target we should choose the model with the
highest PSIS-LOO estimate. This is called elpd$\_$loo which stands for "expected log predictive density" for
the loo that is reliable i.e. all k $<$ 0.5.


\section{Results}

\subsection{Separate Model}
\subsubsection{Stan Code}

\subsubsection{Convergence Statistics}

As seen from the values below that the model has converged as Rhat values are below 1.01.

\begin{figure}[h]
\centerline{\includegraphics[width=300pt]{../img/separate_model_print.png}}
  \caption{Separate model}
  \label{Figure 3}
\end{figure}

\subsubsection{Loo}

As seen from the values and plot below that the k values are all less than 0.5, hence a reliable model.

```{r echo=FALSE}
print(loo_separate_data)
plot(loo_separate_data)
```


\subsubsection{Plots}
```{r echo=FALSE}
mu = matrix(data = NA, nrow = 49, ncol = 4000)

for(i in seq(1:number_of_cities)){
  
  a = ((i-1)*49) +1
  b = i*49
  
  x_temp = x[a:b]
  y_temp = y[a:b]
  
  for(t in seq(from = 1970, to =2019, by =1)){
    mu[t-1970,] =  fit_separate_data$alpha[,i] + ((t-1970) * fit_separate_data$beta[,i])
  }

  mu_quan = colQuantiles(t(mu), probs = c(0.05, 0.5, 0.95))
  
  df_post = data.frame(quan_05=mu_quan[,1], quan_50=mu_quan[,2], quan_95=mu_quan[,3], x=x_temp)
  
  print(
    ggplot(df_post, aes(x=x_temp, y=y_temp)) +
    geom_point(color="darkblue") +
    geom_line(aes(y = quan_05), color = "darkred", linetype="twodash", size = 1.1) +
    geom_line(aes(y = quan_50), color="darkred", linetype = "solid", size = 1.2) +
    geom_line(aes(y = quan_95), color="darkred", linetype="twodash", size = 1.1)+
       labs(y = 'Water level', x= "Year", title = city_names[i]) +
    guides(linetype = F)
  )
}
```

\subsubsection{Analysis}





\subsection{Hierarchical Model}
\subsubsection{Stan Code}



\subsubsection{Convergence Statistics}

As seen from the values below that the model has converged as Rhat values are below 1.01.

\begin{figure}[h]
\centerline{\includegraphics[width=300pt]{../img/hierarchical_model_print.png}}
  \caption{Hierarchical model}
  \label{Figure 3}
\end{figure}

\subsubsection{Loo}

As seen from the values and plot below that the k values are all less than 0.5, hence a reliable model.

```{r echo=FALSE}
print(loo_hierarchical_data)
plot(loo_hierarchical_data)
```

\subsubsection{Plots}

```{r echo=FALSE}
mu = matrix(data = NA, nrow = 49, ncol = 4000)

for(i in seq(1:number_of_cities)){
  
  a = ((i-1)*49) +1
  b = i*49
  
  x_temp = x[a:b]
  y_temp = y[a:b]
  
  for(t in seq(from = 1970, to =2019, by =1)){
    mu[t-1970,] =  fit_hierarchical_data$alpha[,i] + ((t-1970) * fit_hierarchical_data$beta[,i])
  }

  mu_quan = colQuantiles(t(mu), probs = c(0.05, 0.5, 0.95))
  
  df_post = data.frame(quan_05=mu_quan[,1], quan_50=mu_quan[,2], quan_95=mu_quan[,3], x=x_temp)
  
  print(
    ggplot(df_post, aes(x=x_temp, y=y_temp)) +
    geom_point(color="darkblue") +
    geom_line(aes(y = quan_05), color = "darkred", linetype="twodash", size = 1.1) +
    geom_line(aes(y = quan_50), color="darkred", linetype = "solid", size = 1.2) +
    geom_line(aes(y = quan_95), color="darkred", linetype="twodash", size = 1.1)+
       labs(y = 'Water level', x= "Year", title = city_names[i]) +
    guides(linetype = F)
  )
}
```


\subsubsection{Analysis}





\subsection{Posterior Predictive checking}

Several replicated datasets were created and then these distributions were compared to the distribution of the data visually. It can be seen that the model fits the data quite well except around the positive values. Here the data is not normally distributed and there is a disproportionate amount of values around 0 compared to what a normal distribution would have. This could mean that the linear model is not optimal for this problem and that instead some form of exponential model should be used to capture this disproportionate amount of higher values. Based on this it might be possible that the sea level is increasing with an exponential trend in the Baltic Sea.


\subsubsection{Poster Predictive Checking for Separate Model}
```{r echo=FALSE}
y_rep <- as.matrix(fit_separate, pars = "y_rep")
  
for (i in seq(1:number_of_cities)){
  x_temp = x
  y_temp = y
  
  a = ((i-1)*49) +1
  b = i*49
  
  print(
    ppc_dens_overlay(y[a:b], y_rep[1:1000, a:b])
  )
}
```

\subsubsection{Poster Predictive Checking for Hierarchical Model}
```{r echo=FALSE}
y_rep <- as.matrix(fit_hierarchical, pars = "y_rep")
  
for (i in seq(1:number_of_cities)){
  x_temp = x
  y_temp = y
  
  a = ((i-1)*49) +1
  b = i*49
  
  print(
    ppc_dens_overlay(y[a:b], y_rep[1:1000, a:b])
  )
}
```


\section{Model Comparison}

Hierarchical better than Separate. 
```{r echo=FALSE}
compare(loo_hierarchical_data, loo_separate_data)
```













\section{Conclusion}
The threat of global warming and climate change is present in every place on the globe. It can be social, political or immediate threat from changing weather conditions. In Finland we are in lucky position that the changing conditions are not yet severe but the future changes are not yet known. The sea level rise in Finland is not a huge factor although in Gulf of Bothnia the sea level rises in some places 5 millimeters a year. However because of the Ice Age, the ground rises in that area almost 8 millimeters a year in that area. Also in southern Finland the Post-glacial rebound is about 4 to 5 millimeters a year and the in the Gulf of Finland the sea levels are actually decreasing.

The fact that the sea level rise is not a factor in Finland does not of course mean that the climate change does not affect to Finland and we could ignore it. The fight against climate change should be a shared problem between the nations and none should or could ignore the future of our home.

\section{Improvement to the Project}
We settled into the linear regression quite early going into the project. We tried some polynomial fittings also but they started to over fit quite severely. To improve this project we could have maybe tried more to find better models to fit into our problem. However we are quite happy with the results we got out of the data.
