---
title: "Impact of Global Warming on Finland"
output: pdf_document
include: pdf_document

---
\newpage

\section * {Appendix}

## Data pre-processing script in Python

```
import os.path

import pandas as pd


def process_data(data_path, mean_sea_level_path, output_file_path, yearly_means_output_path):
    print(f"Pre-processing file: {data_path}")

    data = pd.read_csv(data_path)
    mean_sea_level_data = pd.read_csv(mean_sea_level_path)

    # if file exist then don't create a new one and read from it only
    if os.path.isfile(output_file_path):
        n_data = pd.read_csv(output_file_path)
        print(f"Read normalized data from {output_file_path}")
    else:
        n_data = data.copy()

        for i, row in n_data.iterrows():
            if i % 1000 == 0:
                print('\r ready: %.3f%%' % (i / len(n_data) * 100), end=" ")

            # main normalizing step, N2000
            # water_level - mean sea level data for that particular year
            n_data.iat[i, 5] = row['Water level (mm)'] - 
                               mean_sea_level_data.loc[mean_sea_level_data['Year'] ==
                               row['Year']].values[0][1]

        n_data = n_data.rename(columns={'Water level (mm)': 'water_level'})

        n_data.to_csv(output_file_path)

        print(f"\nWrote output to {output_file_path}")

    # finding the mean value of each year
    yearly_means = n_data["water_level"].groupby(n_data["Year"]).mean().dropna()

    with open(yearly_means_output_path, "w") as f:
        f.writelines(pd.Series.to_csv(yearly_means))

    return yearly_means


if __name__ == '__main__':
    data_path = r"../data/Kemi/Kemi.csv"
    mean_sea_level_path = r"../data/Kemi/Kemi_mw_n2000.csv"
    normalized_output_path = r"../data/Kemi/Kemi_normalized.csv"
    yearly_means_output_path = r"../data/Kemi/Kemi_yearly_means.csv"

    yearly_mean = process_data(data_path, 
                              mean_sea_level_path, 
                              normalized_output_path, 
                              yearly_means_output_path)

    print(yearly_mean)

```

# R Code Below for the Analysis

## Loading libraries

```{r message=FALSE, warning=FALSE, results='hide'}
library(rstan)
library(ggplot2)
library(dplyr)
library(bayesplot)
library(loo)
library(matrixStats)
```

## Setting all the paths

```{r warning=FALSE, results='hide', message=FALSE}
main_data_dir = "../data"
separate_stan_model = "../model/separate.stan"
hierarchical_stan_model = "../model/hierarchical.stan"
```

## Loading data once it has been pre-processed

```{r warning=FALSE, results='hide', message=FALSE}
combineListsAsOne <-function(list1, list2){
  n <- c()
  for(x in list1){
    n<-c(n, x)
  }
  for(y in list2){
    n<-c(n, y)
  }
  return(n)
}
number_of_cities = 0
x = list()
y = list()
ii = list()
city_names = list()
for (data_file in list.files(main_data_dir, 
                             pattern="*_yearly_means.csv", 
                             full.names=TRUE, 
                             recursive = TRUE)){
  
  preprocessed_data = read.csv(data_file, header=FALSE)
  
  c_name = strsplit(strsplit(data_file, "/")[[1]][4],"_")[[1]][1]
  city_names[number_of_cities + 1] = c_name
    
  x = combineListsAsOne(x, preprocessed_data[,1])
  y = combineListsAsOne(y, preprocessed_data[,2])
  number_of_cities = number_of_cities + 1
  
  for (j in seq(preprocessed_data[,1])){
    ii = combineListsAsOne(ii, number_of_cities)
  }
}
```

## Plotting the data

```{r warning=FALSE, results='hide', message=FALSE}
data_plot = list(
    'N'= 49,
    'x'= x[1:49],
    'y'= y[1:49],
    'k' = number_of_cities
)
for (i in seq(1:number_of_cities)){
  x_temp = x
  y_temp = y
  
  a = ((i-1)*49) +1
  b = i*49
}
```

## Preparing data for Stan model

```{r warning=FALSE, results='hide', message=FALSE}
data = list(
    'N'= length(x),
    'x'= x,
    'y'= y,
    'k' = number_of_cities,
    'i' = ii
)
```

## Separate Stan Model

```{r warning=FALSE, results='hide', message=FALSE}
fit_separate = stan(separate_stan_model, data=data)
fit_separate_data = extract(fit_separate, permuted=T)
```

## Plots for all cities with slope line and quantiles after Stan fit for Separate Model

```{r warning=FALSE, results='hide', message=FALSE}
mu = matrix(data = NA, nrow = 49, ncol = 4000)

for(i in seq(1:number_of_cities)){
  
  a = ((i-1)*49) +1
  b = i*49
  
  x_temp = x[a:b]
  y_temp = y[a:b]
  
  for(t in seq(from = 1970, to =2019, by =1)){
    mu[t-1970,] =  fit_separate_data$alpha[,i] + ((t-1970) * 
                                                    
                                                    fit_separate_data$beta[,i])
  }

  mu_quan = colQuantiles(t(mu), probs = c(0.05, 0.5, 0.95))
  
  df_post = data.frame(quan_05=mu_quan[,1],
                       
                       quan_50=mu_quan[,2],
                       
                       quan_95=mu_quan[,3],
                       
                       x=x_temp)
}
```

## LOO Fitting for Separate Model

```{r warning=FALSE, results='hide', message=FALSE}
loo_separate_data = loo(fit_separate)
```

## Hierarchical Stan Model

```{r warning=FALSE, results='hide', message=FALSE}
fit_hierarchical = stan(hierarchical_stan_model, data=data)
fit_hierarchical_data = extract(fit_hierarchical, permuted=T)
```

## Plots for all cities with slope line and quantiles after Stan fit for Hierarchical Model

```{r warning=FALSE, results='hide', message=FALSE}
mu = matrix(data = NA, nrow = 49, ncol = 4000)

for(i in seq(1:number_of_cities)){
  
  a = ((i-1)*49) +1
  b = i*49
  
  x_temp = x[a:b]
  y_temp = y[a:b]
  
  for(t in seq(from = 1970, to =2019, by =1)){
    mu[t-1970,] =  fit_hierarchical_data$alpha[,i] + ((t-1970) * fit_hierarchical_data$beta[,i])
  }

  mu_quan = colQuantiles(t(mu), probs = c(0.05, 0.5, 0.95))
  
  df_post = data.frame(quan_05=mu_quan[,1], quan_50=mu_quan[,2], quan_95=mu_quan[,3], x=x_temp)
}
```

## Loo
```{r warning=FALSE, results='hide', message=FALSE}
loo_hierarchical_data = loo(fit_hierarchical)
```

## Posterior Predictive Checking for Separate Model

```{r warning=FALSE, results='hide', message=FALSE}
y_rep <- as.matrix(fit_separate, pars = "y_rep")
  
for (i in seq(1:number_of_cities)){
  x_temp = x
  y_temp = y
  
  a = ((i-1)*49) +1
  b = i*49
}
```

## Posterior Predictive Checking for Hierarchical Model

```{r warning=FALSE, results='hide', message=FALSE}
y_rep <- as.matrix(fit_hierarchical, pars = "y_rep")
  
for (i in seq(1:number_of_cities)){
  x_temp = x
  y_temp = y
  
  a = ((i-1)*49) +1
  b = i*49
}
```


### Model Checking

```{r warning=FALSE, results='hide', message=FALSE}
compare(loo_hierarchical_data, loo_separate_data)
```


\newpage
<center>
# Impact of Global Warming on Finland
</center>
\section{Introduction}
Today, one of the biggest threat to the society is global warming. Sea level rise is the immediate result of global warming as it melts the ice sheets and  expands the water volume. Cities like Venice, New Orleans and Osaka are expected to experience major problems with the rise in sea level in near future. 

Venice is already facing major problems. 

\begin{figure}[h!]
\centerline{\includegraphics[width=200pt]{../img/venice.png}}
  \caption{Venice Floods}
  \label{Figure 1}
\end{figure}


Goal of the project is to examine how much the sea level rise affects the coastal cities in Finland. For purpose of the study, four cities from coastal Finland were selected and the data is from January 1970 to November 2019 is used. Linear regression with Gaussian noise,  $Error \sim N(0, \sigma^2)$, is fitted on the data to check the threat on Finnish cities.

The further sections discuss the data and its pre-processing followed by a discussion on priors, Stan models, convergence statistics and the results obtained. The report is concluded with the findings and possible future work.

\section{Data}

To find out about the sea level rise in Finland, hostorical data is needed about the same. In Finland, the \href{https://ilmatieteenlaitos.fi/havaintojen-lataus#!/}{Finnish Meteorologigal Institute} provides open data about the weather. It is an easy portal to download the data needed for the study. The data from January 1970 to November 2019 is taken for the study. The cities are selected in a way that they are evenly distributed around the coast line so that the study covers the overall situation in Finland. The selected cities are Kemi, Turku, Helsinki and Oulu. The selected cities on map is shown in Figure 2.

The data is useful for weather analysis and is available openly so the results can be made public as the data is public. 

\begin{figure}
\centerline{\includegraphics[width=150pt]{../img/map.png}}
  \caption{Cities selected for the study}
  \label{Figure 2}
\end{figure}

\newpage

\subsection{Format of Data}
Data by Finnish Meteorologigal Institute is collected every hour for every day throughout the year, i.e. 24 * 30 * 365, readings are available for each city per year. A portion of this raw data is shown in Figure 3.

\begin{figure}[h]
\centerline{\includegraphics[width=300pt]{../img/data_example.png}}
  \caption{Raw Data}
  \label{Figure 3}
\end{figure}

\subsection{Data pre-processing}

Once the data is downloaded, it is essential to know the zero of the scale of measurement i.e. the height system in which the values is given. In Finland there are a number of systems available from which N2000 is used.

Thus, the data is normalized using N2000 data which is available on the Finnish Meteorologigal Institute  website,  \href{https://en.ilmatieteenlaitos.fi/theoretical-mean-sea-level}{N2000 data}

Also, the data is too accurate so the mean of sea level for each year is calculated.

The whole data pre-processing script (written in Python) can be found in the Appendix section.

\subsection{Processed Data}

After pre-processing the data consists of one mean sea-level value per year. The example can be found in Figure 4. Visual representation of this processed data is plotted below for all the 4 cities.

\begin{figure}[h]
\centerline{\includegraphics[width=300pt]{../img/processed_data.png}}
  \caption{Processed Data}
  \label{Figure 3}
\end{figure}

```{r echo=FALSE}
for (i in seq(1:number_of_cities)){
  x_temp = x
  y_temp = y
  
  a = ((i-1)*49) +1
  b = i*49
  
  print(ggplot() +
    geom_point(aes(x_temp[a:b], y_temp[a:b]), data = data.frame(data_plot), size = 1) +
    labs(y = 'Water level', x= "Year", title = city_names[i]) +
    guides(linetype = F))
}
```


\section{Project Workflow}
In this section a general overview of what is done in the project is provided and it follows the Bayesian Work-flow. The steps are shown below:
\begin{itemize}
    \item Define the Bayesian Models
    \item Perform sensitivity analysis of priors and choose appropriate prior(s)
    \item Define the models and fit the data
    \item Check Convergence Statistics
    \item Perform Posterior Predictive Analysis
    \item Visualize model through plots
    \item Perform PSIS LOO cross validation
    \item Perform Model evaluation
\end{itemize}

\subsection{Models}
A \textbf{gaussian linear model} to the time series data from 1970 to 2019 is fitted. Setting year 1970 as origin makes the slope parameter relative to year 1970 and the intercept modelled for year 1970. Since measurements are collected from 4 different geographical locations it is decided to try a \textbf{hierarchial model} in addition to a \textbf{separate model}.


The Guassian Linear Model is defined as : 
\textbf{$y = \beta x + \alpha$
where $\beta$ is the slope, $\alpha$ is a constant and $x$ is the year number.} 


Different priors for both the models were tested which is described in the next section. Based on the results of these, suitable weakly informative priors are chosen for the models.

\subsection{Sensitivity Analysis of Priors}
\subsubsection{Informative Priors}
According to \href{https://sealevel.nasa.gov/understanding-sea-level/key-indicators/global-mean-sea-level/}{NASA}, the global mean sea level rise is \textbf{3.3 mm} which can be used as a prior. However, it is a very informative prior so weakly informative priors were defined to fit the models. 
\subsubsection{Weakly Informative Priors}
The weakly informative priors tested are:
\begin{enumerate}
    \item $N(3.3,100)$
    \item $N(0,100)$
    \item $N(0,10)$
\end{enumerate}
However it was decided to use $N(0,10)$ as the prior for both the models.

\section{Model Fitting}
\subsection{Separate Model}
A weakly informative prior which is $N \sim (0,10)$ is being used, so 0 is the prior mean for the slope and 10 is the standard deviation. No prior is given to alpha because the slope should approximate the data as well as possible. Figure 5 shows the visual representation of the separate model.


\begin{figure}[h]
\centerline{\includegraphics[width=100pt]{../img/separatemodel.jpg}}
  \caption{Separate Model}
  \label{Figure 4}
\end{figure}

\subsubsection{Priors}
$p(\beta_s) \sim N(0,10)$
\subsubsection{Likelihood}
$p(y|\theta) \sim N(\alpha_s + \beta_s (\textbf{x}-1970) | \sigma_s)$

\subsection{Hierarchical Model}
As data is available from 4 different locations it makes sense to try a hierarchial model. A prior distribution is used, based on the same global data as the seperate model $N \sim (0,10)$. $\beta_0$ is used as a common mean for modelling the slope for each location. Then another hyperprior $p(\sigma_0) \sim N(0,10)$ is used as a common variance for the slopes. Figure 6 shows the visual representation of the hierarchical model.


\begin{figure}[h]
\centerline{\includegraphics[width=150pt]{../img/hierarchicalmodel.jpg}}
  \caption{Hierarchical Model}
  \label{Figure 4}
\end{figure}

\subsubsection{Hyperpriors}
$p(\beta_0) \sim N(0|10)$

$p(\sigma_0) \sim N(0 | 10)$

\subsubsection{Priors}
$p(\beta_g) \sim N(\beta_0 | \sigma_0)$

\subsubsection{Likelihood}
$p(y|\theta) \sim N(\alpha_g + \beta_g (\textbf{x}-1970) | \sigma)$



\subsection{Convergence Statistics in Stan}
\begin{table}[h]
    \centering
    \begin{tabular}{||c|c|c|c|c|c|c|c||}
           \hline
            Variable & mean & se\_mean & sd &2.50\% & 97.50\% & n\_eff & Rhat \\
            \hline
             &  &  & & & & &

    \end{tabular}
    \caption{Sample table layout for convergence statistics}
    \label{tab:my_label}
\end{table}

For each parameter (variable) of the model, the format as shown in Table 1 shows:
\begin{itemize}
    \item The mean column shows the mean for each variable of all the posterior draws of that variable generated in Markov Chain Monte Carlo (MCMC) simulation.
    \item The se$\_$mean column shows standard errors (se) for the posterior means. Standard error of the mean (SEM) depends both on the standard deviation (SD) and the sample size (n): SE = SD/$\sqrt{n}$.
    \item The sd column tells the standard deviation (SD). SD is a measure of variability or dispersion.
    \item The 2.5 $\%$ column reports the value corresponding the lower bound of 95$\%$  confidence interval.
    \item The 97.5 $\%$  column reports the value corresponding the upper bound of 95$\%$  confidence interval.
    \item The n$\_$eff  column tells the effective number of simulation draws, called the effective sample size, n$\_$eff
    \item The Rhat column reports us the values of Rˆ that is our convergence statistic, Rˆ. Here we will consider the threshold of 1.01 for the condition of Rˆ being ’near’ 1 . If Rˆ $>$ 1.01, we consider that the chains have not converged, while if Rˆ $<$ 1.01 we consider that the chains have probably converged and estimates are reliable.
\end{itemize}

The Results section will show the details of the convergence statistics for both Separate and Hierarchical model. 

The default tree depth in Stan is being used for HMC convergence diagnostic. 

\subsection{Leave One out cross validation}
Leave-one-out cross-validation (LOO-CV or LOO) is a method to evaluate the predictive performance of
fitted models for a data set. LOO-CV is an approach for estimating pointwise out-of-sample prediction
accuracy from the fitted models using the log-likelihood assessed at the posterior simulations of the parameter
values.

Here we use Pareto smoothed importance sampling (PSIS) LOO (PSIS-LOO) method for computing ap-
proximate LOO-CV given the posterior draws of the parameters. PSIS is a new approach that makes it
possible to compute LOO using importance weights that would otherwise be unstable. PSIS fits a Pareto
distribution to the upper tail of the distribution of the importance weights, and in this way provides relatively
accurate and reliable estimate.

PSIS-LOO estimate is the sum of the LOO log predictive densities. The reliability of the PSIS-LOO estimates
are assessed for a fitted model based on the k-values that are the estimated Pareto tail indices. The PSIS-LOO
estimate can be considered reliable if all k-values are k $<$ 0.5. Otherwise, we have a concern that the
PSIS-LOO estimate may be biased i.e. it is possible that the estimate is too optimistic, overestimating the
predictive accuracy of the fitted model.

When we are comparing different models which have the same target we should choose the model with the
highest PSIS-LOO estimate. This is called elpd$\_$loo which stands for "expected log predictive density" for
the loo that is reliable i.e. all k $<$ 0.5.


\section{Results}

\subsection{Separate Model}
\subsubsection{Stan Code}
```
data {
  int N;
  vector[N] y;
  vector[N] x;
  int k;
  int i[N];
}

parameters {
  vector[k] beta;
  vector[k] alpha;
  vector<lower=0>[k] sigma;
}

model {
  beta ~ normal(0,10);
  y ~ normal(alpha[i] + beta[i] .* (x-1970), sigma[i]);
}

generated quantities{
  vector[N] log_lik;
  vector[N] y_rep;
 
 for (j in 1:N) {
   log_lik[j] = normal_lpdf(y[j] | alpha[i[j]] + beta[i[j]] .* (x[j]-1970), sigma[i[j]]);
   y_rep[j] = normal_rng(alpha[i[j]] + beta[i[j]] .* (x[j]-1970), sigma[i[j]]);
 }
}
```
\subsubsection{Convergence Statistics}

Model convergences means that the sampling chains have merged and the samples start representing the sampled distribution. The  R-hat  values tells how well the model has convergenced with respect to a parameter. It generally should be 0.9 and 1.1. It can be seen from Figure 7, the  R-hat  values are all very close to 1 and therefore it can be said that the separate model has converged.

\begin{figure}[h]
\centerline{\includegraphics[width=400pt]{../img/separate_model_print.png}}
  \caption{Convergence Statistics for Separate Model}
  \label{Figure}
\end{figure}

\newpage
\subsubsection{Loo}

It can be seen from the values and plot below that almost all k values are less than 0.5 and a very few are above 0.5 which are biased. Although it is still a reliable model. Further hierarchical model is also calculated for comparison.

```{r echo=FALSE}
print(loo_separate_data)
plot(loo_separate_data)
```


\subsubsection{\textbf{Plots after Stan fit}}

```{r echo=FALSE}
mu = matrix(data = NA, nrow = 49, ncol = 4000)

for(i in seq(1:number_of_cities)){
  
  a = ((i-1)*49) +1
  b = i*49
  
  x_temp = x[a:b]
  y_temp = y[a:b]
  
  for(t in seq(from = 1970, to =2019, by =1)){
    mu[t-1970,] =  fit_separate_data$alpha[,i] + ((t-1970) * fit_separate_data$beta[,i])
  }

  mu_quan = colQuantiles(t(mu), probs = c(0.05, 0.5, 0.95))
  
  df_post = data.frame(quan_05=mu_quan[,1], quan_50=mu_quan[,2], quan_95=mu_quan[,3], x=x_temp)
  
  print(
    ggplot(df_post, aes(x=x_temp, y=y_temp)) +
    geom_point(color="darkblue") +
    geom_line(aes(y = quan_05), color = "darkred", linetype="twodash", size = 1.1) +
    geom_line(aes(y = quan_50), color="darkred", linetype = "solid", size = 1.2) +
    geom_line(aes(y = quan_95), color="darkred", linetype="twodash", size = 1.1)+
       labs(y = 'Water level', x= "Year", title = city_names[i]) +
    guides(linetype = F)
  )
}
```

\subsection{Hierarchical Model}

\subsubsection{Stan Code}

```
data {
  int N;
  vector[N] y;
  vector[N] x;
  int k;
  int i[N];
}

parameters {
  vector[k] beta;
  vector[k] alpha;
  real <lower=0> sigma;
  real <lower=0> sigma0;
  real beta0;
}

model {
  beta0 ~ normal(0,10);
  sigma0 ~ normal(0, 10);
  beta ~ normal(beta0, sigma0);
  y ~ normal(alpha[i] + beta[i] .* (x-1970), sigma);
}

generated quantities{
  vector[N] log_lik;
  vector[N] y_rep;
 
 for (j in 1:N) {
   log_lik[j] = normal_lpdf(y[j] | alpha[i[j]] + beta[i[j]] .* (x[j]-1970), sigma);
   y_rep[j] = normal_rng(alpha[i[j]] + beta[i[j]] .* (x[j]-1970), sigma);
 }
}
```

\subsubsection{Convergence Statistics}

Model convergences means that the sampling chains have merged and the samples start representing the sampled distribution. The  R-hat  values tells how well the model has convergenced with respect to a parameter. It generally should be 0.9 and 1.1. It can be seen from Figure 8, the  R-hat values are all very close to 1 and therefore it can be said that the hierarchical model has converged.

\begin{figure}[h]
\centerline{\includegraphics[width=400pt]{../img/hierarchical_model_print.png}}
  \caption{Convergence Statistics for Hierarchical model}
  \label{Figure 3}
\end{figure}

\subsubsection{Loo}

It can be seen from the values and plot below that almost all k values are less than 0.5 and almost no values are above 0.5. So it is a reliable model and performs marginally better than the Separate model computed above.

```{r echo=FALSE}
print(loo_hierarchical_data)
plot(loo_hierarchical_data)
```

\subsubsection{\textbf{Plots after Stan fit}}

```{r echo=FALSE}
mu = matrix(data = NA, nrow = 49, ncol = 4000)

for(i in seq(1:number_of_cities)){
  
  a = ((i-1)*49) +1
  b = i*49
  
  x_temp = x[a:b]
  y_temp = y[a:b]
  
  for(t in seq(from = 1970, to =2019, by =1)){
    mu[t-1970,] =  fit_hierarchical_data$alpha[,i] + ((t-1970) * fit_hierarchical_data$beta[,i])
  }

  mu_quan = colQuantiles(t(mu), probs = c(0.05, 0.5, 0.95))
  
  df_post = data.frame(quan_05=mu_quan[,1], quan_50=mu_quan[,2], quan_95=mu_quan[,3], x=x_temp)
  
  print(
    ggplot(df_post, aes(x=x_temp, y=y_temp)) +
    geom_point(color="darkblue") +
    geom_line(aes(y = quan_05), color = "darkred", linetype="twodash", size = 1.1) +
    geom_line(aes(y = quan_50), color="darkred", linetype = "solid", size = 1.2) +
    geom_line(aes(y = quan_95), color="darkred", linetype="twodash", size = 1.1)+
       labs(y = 'Water level', x= "Year", title = city_names[i]) +
    guides(linetype = F)
  )
}
```

\subsection{Effective Sample Size Diagnostic (ESS)}

As it can be seen from the loo results table above, ESS, p-loo in table, for hierarchical model is around 12 whereas for separate it is around 23. Hence,  hierarchical model performs better than separate model as it uses less number of parameters.


\section{Model Comparison}

Hierarchical model performs better than Separate model for the following reasons:

1. K-hat values in hierarchical model are very reliable
2. ELPD value is more for hierarchical model
3. Effective number of parameters p_loo is less for hierarchical model

These results are separately printed above and below is the result obtained from the compare function in loo. 

```{r echo=FALSE}
compare(loo_hierarchical_data, loo_separate_data)
```


\subsection{Posterior Predictive checking}

Several replicated datasets are created and then these distributions are compared to the distribution of the data visually. It can be seen that the model fits the data quite well except around the positive values. Here the data is not normally distributed and there is a disproportionate amount of values around 0 compared to what a normal distribution would have. This could mean that the linear model is not optimal for this problem and that instead some form of exponential model should be used to capture this disproportionate amount of higher values. Based on this it might be possible that the sea level is increasing with an exponential trend in the Baltic Sea.


\subsubsection{Poster Predictive Checking for Separate Mode for each city}

```{r echo=FALSE}
y_rep <- as.matrix(fit_separate, pars = "y_rep")
  
for (i in seq(1:number_of_cities)){
  x_temp = x
  y_temp = y
  
  a = ((i-1)*49) +1
  b = i*49
  
  print(
    ppc_dens_overlay(y[a:b], y_rep[1:1000, a:b])
  )
}
```

\subsubsection{Poster Predictive Checking for Hierarchical Model for each city}
```{r echo=FALSE}
y_rep <- as.matrix(fit_hierarchical, pars = "y_rep")
  
for (i in seq(1:number_of_cities)){
  x_temp = x
  y_temp = y
  
  a = ((i-1)*49) +1
  b = i*49
  
  print(
    ppc_dens_overlay(y[a:b], y_rep[1:1000, a:b])
  )
}
```

\section{Conclusion}
In Finland we are in lucky position that the changing conditions are not yet severe but the future changes are not yet known. The sea level rise in Finland is not a huge factor although in Gulf of Bothnia the sea level rises in some places 5 millimeters a year. However because of the Ice Age, the ground rises in that area almost 8 millimeters a year in that area. Also in southern Finland the Post-glacial rebound is about 4 to 5 millimeters a year and in the Gulf of Finland the sea levels are actually decreasing.

The fact that the sea level rise is not a huge factor in Finland does not, of course, mean that the climate change does not affect Finland and it can be ignored.

The summary of the results for various cities are shown in Figure 9. 

\begin{figure}[h]
\centerline{\includegraphics[width=200pt]{../img/model_values.png}}
  \caption{Slope values for all cities from the two models}
  \label{Figure 3}
\end{figure}

According to NASA, the average rise in water-level is 3.3 mm. Considering it, we can conclude Figure 10.


\begin{figure}[h]
\centerline{\includegraphics[width=200pt]{../img/city_threat.png}}
  \caption{Analysis on the results obtained}
  \label{Figure 3}
\end{figure}


\section{Improvement to the Project}
We settled into the linear regression quite early going into the project. We tried some polynomial fittings also but they started to over fit quite severely. To improve this project we could have maybe tried more to find better models to fit into our problem. However we are quite happy with the results we got out of the data.